{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstructBLIP Image Captioning Notebook\n",
    "This notebook loads the `Salesforce/instructblip-vicuna-7b` model to generate captions for uploaded images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q git+https://github.com/huggingface/peft.git\n",
    "!pip install -q git+https://github.com/huggingface/huggingface_hub.git"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. Import modules and upload images\n",
    "import os\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from google.colab import files\n",
    "\n",
    "# Upload image files\n",
    "uploaded = files.upload()\n",
    "image_paths = list(uploaded.keys())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. Load InstructBLIP model and processor\n",
    "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
    "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-vicuna-7b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5. Generate image descriptions\n",
    "results = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image_name = os.path.basename(image_path)\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize((384, 384))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open image {image_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    creation_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    prompt = \"You are a helpful AI assistant. Describe this image in detail.\"\n",
    "\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "        description = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "    results.append({\n",
    "        \"Filename\": image_name,\n",
    "        \"Capture Time\": creation_datetime,\n",
    "        \"Description\": description\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 6. Save to CSV and download\n",
    "df.to_csv(\"instructblip_results.csv\", index=False)\n",
    "files.download(\"instructblip_results.csv\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "instructblip_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
