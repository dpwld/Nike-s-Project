{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# InternLM assistant Text description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Install\n",
        "!pip install -q transformers accelerate pandas openpyxl pillow bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import Library\n",
        "import torch, os, glob\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    InstructBlipProcessor, InstructBlipForConditionalGeneration,\n",
        "    AutoTokenizer, AutoModelForCausalLM\n",
        ")\n",
        "from bert_score import score as bertscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import InternLM\n",
        "blip_id = \"Salesforce/instructblip-vicuna-7b\"\n",
        "llm_id = \"internlm/internlm-chat-7b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "blip_processor = InstructBlipProcessor.from_pretrained(blip_id)\n",
        "blip_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
        "    blip_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_id, trust_remote_code=True)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_id, trust_remote_code=True, torch_dtype=torch.float16\n",
        ").cuda()\n",
        "llm_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Upload fewshot\n",
        "excel_path = \"/content/fewshot_data.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "answer_cols = [\"answer_1\", \"answer_2\", \"answer_3\", \"answer_4\", \"answer_5\", \"answer_6\"]\n",
        "image_paths = sorted(glob.glob(\"/content/*.[jpJP][pnNP]*[gG]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Definition\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize((384, 384))\n",
        "    prompt = \"You are a helpful assistant. Describe this image in detail.\"\n",
        "    inputs = blip_processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = blip_model.generate(**inputs)\n",
        "    caption = blip_processor.batch_decode(output, skip_special_tokens=True)[0].strip()\n",
        "    return caption.replace(prompt, \"\").strip()\n",
        "\n",
        "def generate_llm_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs, max_new_tokens=100, do_sample=True,\n",
        "            temperature=0.7, top_p=0.9\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Generate description\n",
        "captions, llm_responses, answer_lists = [], [], []\n",
        "\n",
        "for idx, image_path in enumerate(image_paths):\n",
        "    try:\n",
        "        caption = generate_caption(image_path)\n",
        "        captions.append(caption)\n",
        "\n",
        "        prompt = f\"{caption}\\nAnswer:\"\n",
        "        response = generate_llm_response(prompt)\n",
        "        llm_responses.append(response)\n",
        "\n",
        "        row_answers = [df.loc[idx, col] for col in answer_cols if pd.notna(df.loc[idx, col]) and str(df.loc[idx, col]).strip()]\n",
        "        answer_lists.append(row_answers if row_answers else [\"[EMPTY]\"])\n",
        "\n",
        "        print(f\"{os.path.basename(image_path)} 완료\")\n",
        "    except Exception as e:\n",
        "        print(f\"{os.path.basename(image_path)} 실패: {e}\")\n",
        "        captions.append(\"[ERROR]\")\n",
        "        llm_responses.append(f\"[ERROR] {e}\")\n",
        "        answer_lists.append([\"[EMPTY]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BERTScore\n",
        "best_scores = {\n",
        "    \"precision\": [], \"recall\": [], \"f1\": [], \"matched_answer\": []\n",
        "}\n",
        "\n",
        "for refs, cand in zip(answer_lists, llm_responses):\n",
        "    P, R, F1 = bertscore(cands=[cand]*len(refs), refs=refs, lang=\"ko\", verbose=False)\n",
        "    best_idx = F1.argmax().item()\n",
        "    best_scores[\"precision\"].append(P[best_idx].item())\n",
        "    best_scores[\"recall\"].append(R[best_idx].item())\n",
        "    best_scores[\"f1\"].append(F1[best_idx].item())\n",
        "    best_scores[\"matched_answer\"].append(refs[best_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Display as DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    \"image\": [os.path.basename(p) for p in image_paths],\n",
        "    \"caption\": captions,\n",
        "    \"llm_response\": llm_responses,\n",
        "    \"matched_answer\": best_scores[\"matched_answer\"],\n",
        "    \"bert_precision\": best_scores[\"precision\"],\n",
        "    \"bert_recall\": best_scores[\"recall\"],\n",
        "    \"bert_f1\": best_scores[\"f1\"]\n",
        "})\n",
        "results_df.to_excel(\"/content/llm_result_internlm_chat.xlsx\", index=False)\n",
        "print(\"files.download: /content/llm_result_internlm_chat.xlsx\")"
      ]
    }
  ]
}
