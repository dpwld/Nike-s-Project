{
{
  "nbformat": 4,
  "nbformat_minor": 5,ㅊ
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
 "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLAMA2 assistant Text description"
      ]
    },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Install\n",
        "!pip install -q transformers accelerate bitsandbytes pandas openpyxl pillow bert_score\n",
        "!pip install -q bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import Library\n",
        "import torch, os, glob\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    InstructBlipProcessor, InstructBlipForConditionalGeneration,\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig, pipeline\n",
        ")\n",
        "from bert_score import score as bertscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hugging Face\n",
        "login(\"hf_API KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#LLAMA2\n",
        "blip_processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
        "blip_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/instructblip-vicuna-7b\", torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "llama_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(llama_id)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_id, device_map=\"auto\", quantization_config=bnb_config, torch_dtype=torch.float16\n",
        ")\n",
        "llm_pipe = pipeline(\"text-generation\", model=llama_model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Fewshot data\n",
        "excel_path = \"/content/fewshot_data.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "answer_cols = [\"answer_1\"]\n",
        "image_paths = sorted(glob.glob(\"/content/*.[jpJP][pnNP]*[gG]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function definition\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize((384, 384))\n",
        "    prompt = \"You are a helpful assistant. Describe this image in detail.\"\n",
        "    inputs = blip_processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = blip_model.generate(**inputs)\n",
        "    caption = blip_processor.batch_decode(output, skip_special_tokens=True)[0].strip()\n",
        "    return caption.replace(prompt, \"\").strip()\n",
        "\n",
        "def clean_llama_response(raw_text):\n",
        "    if \"[/INST]\" in raw_text:\n",
        "        return raw_text.split(\"[/INST]\")[-1].strip()\n",
        "    return raw_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Generate text description\n",
        "captions, llm_responses, answer_lists = [], [], []\n",
        "\n",
        "for idx, image_path in enumerate(image_paths):\n",
        "    try:\n",
        "        caption = generate_caption(image_path)\n",
        "        captions.append(caption)\n",
        "\n",
        "        prompt = f\"[INST] <<SYS>> You are a helpful assistant. <</SYS>>\\n\\n{caption} [/INST]\"\n",
        "        output = llm_pipe(prompt, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
        "        response = clean_llama_response(output[0]['generated_text'])\n",
        "\n",
        "        print(f\"{os.path.basename(image_path)} 완료\")\n",
        "    except Exception as e:\n",
        "        caption = \"[ERROR]\"\n",
        "        response = f\"[ERROR] {e}\"\n",
        "        print(f\"{os.path.basename(image_path)} 실패: {e}\")\n",
        "\n",
        "    row_answers = [df.loc[idx, col] for col in answer_cols if pd.notna(df.loc[idx, col]) and str(df.loc[idx, col]).strip()]\n",
        "    answer_lists.append(row_answers if row_answers else [\"[EMPTY]\"])\n",
        "    llm_responses.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BERTScore\n",
        "best_scores = {\n",
        "    \"precision\": [],\n",
        "    \"recall\": [],\n",
        "    \"f1\": [],\n",
        "    \"matched_answer\": []\n",
        "}\n",
        "\n",
        "for refs, cand in zip(answer_lists, llm_responses):\n",
        "    P, R, F1 = bertscore(cands=[cand]*len(refs), refs=refs, lang=\"ko\", verbose=False)\n",
        "    best_idx = F1.argmax().item()\n",
        "\n",
        "    best_scores[\"precision\"].append(P[best_idx].item())\n",
        "    best_scores[\"recall\"].append(R[best_idx].item())\n",
        "    best_scores[\"f1\"].append(F1[best_idx].item())\n",
        "    best_scores[\"matched_answer\"].append(refs[best_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save to CSV\n",
        "results_df = pd.DataFrame({\n",
        "    \"image\": [os.path.basename(p) for p in image_paths],\n",
        "    \"caption\": captions,\n",
        "    \"llm_response\": llm_responses,\n",
        "    \"matched_answer\": best_scores[\"matched_answer\"],\n",
        "    \"bert_precision\": best_scores[\"precision\"],\n",
        "    \"bert_recall\": best_scores[\"recall\"],\n",
        "    \"bert_f1\": best_scores[\"f1\"]\n",
        "})\n",
        "results_df.to_excel(\"/content/llm_result_multi_answer.xlsx\", index=False)\n",
        "print(\"files.download: /content/llm_result_multi_answer.xlsx\")"
      ]
    }
  ]
}
